{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "colab": {
   "name": "Kanye Music Generation.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsgRq-6wwPbA",
    "colab_type": "text"
   },
   "source": [
    "# Libraies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "svZWnIHwwNKo",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Import Necessary Libaries\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "from midi_utils import midiread, midiwrite\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage.io as io\n",
    "from IPython.display import FileLink\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGNqzEZbwJyL",
    "colab_type": "text"
   },
   "source": [
    "# Pre-Defined Parameters\n",
    "This is where we can define some model pramaters before we begin training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uys_7Qbuvbqp",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Customize Training Parameters Here\n",
    "CLIP = 3\n",
    "RNN_TYPE = \"gru\"\n",
    "DROPOUT = \"normal\"\n",
    "DROPOUT_RATE = .5\n",
    "OPTIMIZER_TYPE = \"adam\"\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "criterion_val = nn.CrossEntropyLoss().cuda()\n",
    "criterion_test = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# Customize Outout MIDI File Here \n",
    "SAMPLE_LENGTH = 400\n",
    "TEMPERATURE = .8\n",
    "DT = .25"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss1aYU7G6mdm",
    "colab_type": "text"
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZUDM_M80u5n",
    "colab_type": "text"
   },
   "source": [
    "This Section of code is responsible for converting the .midi files to a numeric representation.\n",
    "Code Taken Directly from https://github.com/jmcarpenter2/music-generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T12:20:52.867674Z",
     "start_time": "2018-11-27T12:20:52.819795Z"
    },
    "id": "60SOACKA6mdo",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "def midi_filename_to_piano_roll(midi_filename):\n",
    "    \n",
    "    midi_data = midiread(midi_filename, dt=0.3)\n",
    "    \n",
    "    piano_roll = midi_data.piano_roll.transpose()\n",
    "    \n",
    "    # Pressed notes are replaced by 1\n",
    "    piano_roll[piano_roll > 0] = 1\n",
    "    \n",
    "    return piano_roll\n",
    "\n",
    "\n",
    "def pad_piano_roll(piano_roll, max_length=132333, pad_value=0):\n",
    "        \n",
    "    original_piano_roll_length = piano_roll.shape[1]\n",
    "    \n",
    "    padded_piano_roll = np.zeros((88, max_length))\n",
    "    padded_piano_roll[:] = pad_value\n",
    "    \n",
    "    padded_piano_roll[:, -original_piano_roll_length:] = piano_roll\n",
    "\n",
    "    return padded_piano_roll\n",
    "\n",
    "\n",
    "class NotesGenerationDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, midi_folder_path, longest_sequence_length=1491):\n",
    "        \n",
    "        self.midi_folder_path = midi_folder_path\n",
    "        \n",
    "        midi_filenames = os.listdir(midi_folder_path)\n",
    "        \n",
    "        self.longest_sequence_length = longest_sequence_length\n",
    "        \n",
    "        midi_full_filenames = map(lambda filename: os.path.join(midi_folder_path, filename),midi_filenames)\n",
    "        \n",
    "        self.midi_full_filenames = list(midi_full_filenames)\n",
    "        \n",
    "        if longest_sequence_length is None:\n",
    "            \n",
    "            self.update_the_max_length()\n",
    "    \n",
    "    \n",
    "    def update_the_max_length(self):\n",
    "        \n",
    "        sequences_lengths = map(lambda filename: midi_filename_to_piano_roll(filename).shape[1],self.midi_full_filenames)\n",
    "        \n",
    "        max_length = max(sequences_lengths)\n",
    "        \n",
    "        self.longest_sequence_length = max_length\n",
    "                \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.midi_full_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        midi_full_filename = self.midi_full_filenames[index]\n",
    "        \n",
    "        piano_roll = midi_filename_to_piano_roll(midi_full_filename)\n",
    "        \n",
    "        # Shifting by one time step\n",
    "        sequence_length = piano_roll.shape[1] - 1\n",
    "        \n",
    "        # Shifting by one time step\n",
    "        input_sequence = piano_roll[:, :-1]\n",
    "        ground_truth_sequence = piano_roll[:, 1:]\n",
    "                \n",
    "        # padding sequence so that all of them have the same length\n",
    "        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length)\n",
    "        \n",
    "        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,max_length=self.longest_sequence_length,pad_value=-100)\n",
    "                \n",
    "        input_sequence_padded = input_sequence_padded.transpose()\n",
    "        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n",
    "        \n",
    "        return (torch.FloatTensor(input_sequence_padded),torch.LongTensor(ground_truth_sequence_padded),torch.LongTensor([sequence_length]) )\n",
    "\n",
    "    \n",
    "def post_process_sequence_batch(batch_tuple):\n",
    "    \n",
    "    input_sequences, output_sequences, lengths = batch_tuple\n",
    "    \n",
    "    splitted_input_sequence_batch = input_sequences.split(split_size=1)\n",
    "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
    "    splitted_lengths_batch = lengths.split(split_size=1)\n",
    "\n",
    "    training_data_tuples = zip(splitted_input_sequence_batch,\n",
    "                               splitted_output_sequence_batch,\n",
    "                               splitted_lengths_batch)\n",
    "\n",
    "    training_data_tuples_sorted = sorted(training_data_tuples,\n",
    "                                         key=lambda p: int(p[2]),\n",
    "                                         reverse=True)\n",
    "\n",
    "    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
    "\n",
    "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
    "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
    "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
    "    \n",
    "    input_sequence_batch_sorted = input_sequence_batch_sorted[:, -lengths_batch_sorted[0, 0]:, :]\n",
    "    output_sequence_batch_sorted = output_sequence_batch_sorted[:, -lengths_batch_sorted[0, 0]:, :]\n",
    "    \n",
    "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
    "    \n",
    "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
    "    lengths_batch_sorted_list = map(lambda x: int(x), lengths_batch_sorted_list)\n",
    "    \n",
    "    return input_sequence_batch_transposed, output_sequence_batch_sorted, list(lengths_batch_sorted_list)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T12:21:07.678124Z",
     "start_time": "2018-11-27T12:20:56.931002Z"
    },
    "id": "BrC-Dpg46mdv",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "outputId": "715fa776-8f33-4d70-e002-7d11baff1064"
   },
   "source": [
    "# Load Training Data\n",
    "trainset = NotesGenerationDataset('./Kanye/train/', longest_sequence_length=None)\n",
    "trainset_loader = data.DataLoader(trainset, batch_size=8,shuffle=True, drop_last=True)\n",
    "\n",
    "# Get Training Data Shape\n",
    "X = next(iter(trainset_loader))\n",
    "print(\"Train Size:\",X[0].shape)\n",
    "print()\n",
    "\n",
    "# Load Validation Set\n",
    "valset = NotesGenerationDataset('./Kanye/valid/', longest_sequence_length=None)\n",
    "valset_loader = data.DataLoader(valset, batch_size=8, shuffle=False, drop_last=False)\n",
    "\n",
    "# Get Validation Set Size\n",
    "X_val = next(iter(valset_loader))\n",
    "print(\"Validation Size:\",X_val[0].shape)\n",
    "print()\n",
    "\n",
    "# Load Test Set\n",
    "testset = NotesGenerationDataset('./Kanye/test/', longest_sequence_length=None)\n",
    "testset_loader = data.DataLoader(testset, batch_size=8, shuffle=False, drop_last=False)\n",
    "\n",
    "# Get Test Set Size\n",
    "X_test = next(iter(testset_loader))\n",
    "print(\"Test Size:\",X_test[0].shape)\n",
    "print()\n",
    "\n",
    "# Load Full Data Set\n",
    "fulldata = NotesGenerationDataset('./Kanye/full_data/', longest_sequence_length=None)\n",
    "full_data_loader = data.DataLoader(fulldata, batch_size=8, shuffle=False, drop_last=False)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Train Size: torch.Size([8, 2643, 88])\n",
      "\n",
      "Validation Size: torch.Size([8, 2643, 88])\n",
      "\n",
      "Test Size: torch.Size([8, 1842, 88])\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNsNSHAh6mgH",
    "colab_type": "text"
   },
   "source": [
    "# RNN\n",
    "Here we build our RNN. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T12:22:33.314323Z",
     "start_time": "2018-11-27T12:22:33.291386Z"
    },
    "id": "Ry9-FNgn6mgI",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "\n",
    "        # Linear Layer\n",
    "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        \n",
    "        # Batch Normalization\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # RNN (GRU or LSTM)\n",
    "        if RNN_TYPE == \"lstm\":\n",
    "          self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        else:\n",
    "          self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "        # Linear Layer\n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        \n",
    "        # Dropout (Alpha or Regular)\n",
    "        if DROPOUT == \"alpha\":\n",
    "          self.dropout = nn.AlphaDropout(DROPOUT_RATE)\n",
    "        else:\n",
    "          self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
    "        batch_size = input_sequences.shape[1]\n",
    "\n",
    "        notes_encoded = self.notes_encoder(input_sequences)\n",
    "        \n",
    "        \n",
    "        # Change Axis\n",
    "        notes_encoded_rolled = notes_encoded.permute(1,2,0).contiguous()\n",
    "        \n",
    "        # Batch Normalization\n",
    "        notes_encoded_norm = self.bn(notes_encoded_rolled)\n",
    "        \n",
    "        # Dropout\n",
    "        notes_encoded_norm_drop = self.dropout(notes_encoded_norm)\n",
    "        \n",
    "        # Return to axis\n",
    "        notes_encoded_complete = notes_encoded_norm_drop.permute(2,0,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Here we run rnns only on non-padded regions of the batch\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(notes_encoded_complete, input_sequences_lengths)\n",
    "        \n",
    "        # RNN\n",
    "        outputs, hidden = self.rnn(packed, hidden)\n",
    "        \n",
    "        # Here we unpack sequence(back to padded)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        # Batch Normalization\n",
    "        outputs_norm = self.bn(outputs.permute(1,2,0).contiguous())\n",
    "        \n",
    "        # Dropout\n",
    "        outputs_drop = self.dropout(outputs_norm)\n",
    "        \n",
    "        # Linear Layer\n",
    "        logits = self.logits_fc(outputs_drop.permute(2,0,1))\n",
    "        logits = logits.transpose(0, 1).contiguous()\n",
    "        \n",
    "        neg_logits = (1 - logits)\n",
    "        \n",
    "        # Since the BCE loss doesn't support masking,crossentropy is used\n",
    "        binary_logits = torch.stack((logits, neg_logits), dim=3).contiguous()\n",
    "        logits_flatten = binary_logits.view(-1, 2)\n",
    "        return logits_flatten, hidden"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MA18J6H1tRv",
    "colab_type": "text"
   },
   "source": [
    "# Parameter Search\n",
    "This is where we try out different parameters. We loop through different hidden layer values, epochs, and learning rates and use the validation set as the way to determine what parameters gave us the best results using Cross Entropy Loss. We also save the optimal model that gave us the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bjYqqqy3fOHc",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Test the model on the validation set\n",
    "def validate(model):\n",
    "    model.eval()\n",
    "    full_val_loss = 0.0\n",
    "    overall_sequence_length = 0.0\n",
    "\n",
    "    for batch in valset_loader:\n",
    "\n",
    "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "        output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n",
    "\n",
    "        input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n",
    "\n",
    "        logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "        loss = criterion_val(logits, output_sequences_batch_var)\n",
    "\n",
    "        full_val_loss += loss.item()\n",
    "        overall_sequence_length += sum(sequences_lengths)\n",
    "\n",
    "    return full_val_loss / (overall_sequence_length * 88)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IkfZBzJ_VWJo",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Function that determines what parameters we should be using.\n",
    "def lrfinder(start, end, trainset_loader):\n",
    "    hiddens = [16, 32, 64 ,128, 256, 512]\n",
    "    learning_rates = np.linspace(start, end, 4)\n",
    "    epochs = 20\n",
    "\n",
    "    denom = (epochs)*len(hiddens)*len(learning_rates)\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_hidden = 0\n",
    "    best_lr = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    train_loss = float(\"inf\")\n",
    "    counter = 0\n",
    "    \n",
    "    \n",
    "    for hidden in hiddens:\n",
    "      for rate in learning_rates:\n",
    "\n",
    "        print(\"Percent Complete: {}%\".format((counter / denom) * 100))\n",
    "        \n",
    "        \n",
    "        model = RNN(input_size=88, hidden_size=hidden, num_classes=88)\n",
    "        model = model.cuda()\n",
    "        model.train()\n",
    "        \n",
    "        parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        \n",
    "        # Pick the Optimzier the user defines\n",
    "        if OPTIMIZER_TYPE == \"sgd\":\n",
    "          optimizer = torch.optim.SGD(model.parameters(), lr=rate)\n",
    "        else:\n",
    "          optimizer = torch.optim.Adam(model.parameters(), lr=rate)\n",
    "\n",
    "        \n",
    "        for i in range(epochs):\n",
    "          model.train()\n",
    "          epoch_loss = [] \n",
    "          for batch in trainset_loader:\n",
    "            \n",
    "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n",
    "            input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "            # Get the Loss\n",
    "            loss = criterion(logits, output_sequences_batch_var)\n",
    "            epoch_loss.append(loss.item())\n",
    "   \n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the Gradients \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "          counter = counter + 1\n",
    "          \n",
    "\n",
    "          val_loss = validate(model)\n",
    "          if val_loss < best_val_loss:\n",
    "            best_epoch = i\n",
    "            best_hidden = hidden\n",
    "            best_lr = rate\n",
    "            best_val_loss = val_loss\n",
    "            train_loss = sum(epoch_loss)/len(trainset_loader)\n",
    "            torch.save(model.state_dict(), 'music_model_padfront_regularized.pth')\n",
    "\n",
    "\n",
    "\n",
    "    return best_epoch, best_hidden, best_lr, best_val_loss, train_loss\n",
    "            "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JfGwjQs5gk-Q",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "best_epoch, best_hidden_size, best_learning_rate, val_loss, train_loss = lrfinder(1e-4, 1e-1*5,trainset_loader)\n",
    "print(\"Best Epoch:\",best_epoch, \"  Best Hidden Size:\",best_hidden_size, \"  Best Learning Rate:\",best_learning_rate) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUeyqJ5Qfz5f",
    "colab_type": "text"
   },
   "source": [
    "## Test Set\n",
    "With our optimal model, we check to see how this model does on the test set. If the test loss isn't too much higher than the validation loss, we continue. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d3smmZA0rAw-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "outputId": "8ed237aa-a091-4eb2-91e0-bc3194502573"
   },
   "source": [
    "test_model = RNN(input_size=88, hidden_size=best_hidden_size, num_classes=88)\n",
    "test_model = test_model.cuda()\n",
    "test_model.load_state_dict(torch.load('music_model_padfront_regularized.pth'))\n",
    "test_model.eval()\n",
    "\n",
    "full_test_loss = 0.0\n",
    "overall_sequence_length = 0.0\n",
    "\n",
    "for batch in testset_loader:\n",
    "\n",
    "    post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "    input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "    output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n",
    "\n",
    "    input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n",
    "\n",
    "    logits, _ = test_model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "    loss = criterion_test(logits, output_sequences_batch_var)\n",
    "\n",
    "    full_test_loss += loss.item()\n",
    "    overall_sequence_length += sum(sequences_lengths)\n",
    "\n",
    "print(\"Validation Loss\", val_loss)\n",
    "print(\"Test Loss\", full_test_loss / (overall_sequence_length * 88))    "
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Train Loss 0.20582804332176843\n",
      "Validation Loss 6.933839898469698e-07\n",
      "Test Loss 8.318411515215776e-07\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXaRNiClfn2J",
    "colab_type": "text"
   },
   "source": [
    "## Model Training\n",
    "Now that we know what parameters to use, we build the model using the optimal parameters on the entire dataset (train, valid, test)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:16:18.949227Z",
     "start_time": "2018-11-26T16:16:18.930281Z"
    },
    "id": "N-P0lhYV6mg6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train_model(learning_rate, epochs_number, hidden):\n",
    "    model = RNN(input_size=88, hidden_size=hidden, num_classes=88)\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    # Determine which Optimzier to use \n",
    "    if OPTIMIZER_TYPE == \"sgd\":\n",
    "      optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    else:\n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    # Loop through different Epoch values    \n",
    "    for epoch_number in range(epochs_number):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # Loop through different Learning rates \n",
    "        for batch in full_data_loader:\n",
    "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n",
    "            input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "            # Get Loss\n",
    "            loss = criterion(logits, output_sequences_batch_var)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip Gradient\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    torch.save(model.state_dict(), 'final_model.pth')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kp6eTR6X0IYU",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_model(best_learning_rate, best_epoch, best_hidden_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEafC2686mhW",
    "colab_type": "text"
   },
   "source": [
    "# Music Generation\n",
    "Now that we have finished training our RNN, we can now use it to generate music!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:25:11.202962Z",
     "start_time": "2018-11-26T16:25:11.189993Z"
    },
    "id": "6Fjx2hfD6mhX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def sample_from_piano_rnn(rnn, sample_length=4, temperature=1, starting_sequence=None):\n",
    "\n",
    "    # Arbitrary Starting Point\n",
    "    if starting_sequence is None:\n",
    "\n",
    "        # Initialize the current sequence         \n",
    "        current_sequence_input = torch.zeros(1, 1, 88)\n",
    "        current_sequence_input[0, 0, 40] = 1\n",
    "        current_sequence_input[0, 0, 50] = 0\n",
    "        current_sequence_input[0, 0, 56] = 0\n",
    "        current_sequence_input = Variable(current_sequence_input.cuda())\n",
    "    \n",
    "    # If we have a given starting point\n",
    "    else:\n",
    "        current_sequence_input = starting_sequence\n",
    "        \n",
    "    # Create the output sequence \n",
    "    final_output_sequence = [current_sequence_input.data.squeeze(1)]\n",
    "\n",
    "    hidden = None\n",
    "\n",
    "    # Sample For the desired length \n",
    "    for i in range(sample_length):\n",
    "\n",
    "        # Use the current sequence to generate a new sequence \n",
    "        output, hidden = rnn(current_sequence_input, [1], hidden)\n",
    "\n",
    "        # Get the probabilites of possible notes from the output and temperature\n",
    "        probabilities = nn.functional.softmax(output.div(temperature), dim=1)\n",
    "\n",
    "        # Sample from the multinomial distribution and set this new sequence to be current sequence \n",
    "        current_sequence_input = torch.multinomial(probabilities.data, 1).squeeze().unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        # Returns tensor \n",
    "        current_sequence_input = Variable(current_sequence_input.float())\n",
    "\n",
    "        # Append this current sequence to our entire output\n",
    "        final_output_sequence.append(current_sequence_input.data.squeeze(1))\n",
    "\n",
    "    \n",
    "    # Finalize the Output and return\n",
    "    sampled_sequence = torch.cat(final_output_sequence, dim=0).cpu().numpy()\n",
    "    \n",
    "    return sampled_sequence"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T16:25:36.632928Z",
     "start_time": "2018-11-26T16:25:34.966547Z"
    },
    "id": "xxu7fQDX6mi2",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "outputId": "1964cae8-f70b-4579-8cf2-587fc1e652db"
   },
   "source": [
    "final_model = RNN(input_size=88, hidden_size=best_hidden_size, num_classes=88)\n",
    "final_model = final_model.cuda()\n",
    "final_model.load_state_dict(torch.load('final_model.pth'))\n",
    "final_model.eval()\n",
    "sample = sample_from_piano_rnn(final_model, sample_length=SAMPLE_LENGTH, temperature=TEMPERATURE,starting_sequence=None).transpose()\n",
    "io.imshow(sample)\n",
    "midiwrite(RNN_TYPE+\" Clip:\"+str(CLIP)+\" Epochs:\"+str(best_epoch)+\" Optimizer:\"+OPTIMIZER_TYPE+\" Hidden:\"+str(best_hidden_size)+\" Dropout:\"+str(DROPOUT)+\" Dropout Rate:\"+str(DROPOUT_RATE)+\" Length:\"+str(SAMPLE_LENGTH)+\" Temperature:\"+str(TEMPERATURE)+\" DT:\"+str(DT)+\".mid\", sample.transpose(), dt=DT)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (notes_encoder): Linear(in_features=88, out_features=16, bias=True)\n",
       "  (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (rnn): GRU(16, 16, num_layers=2)\n",
       "  (logits_fc): Linear(in_features=16, out_features=88, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 13
    }
   ]
  }
 ]
}