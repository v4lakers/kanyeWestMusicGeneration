{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Kanye Music Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsgRq-6wwPbA",
        "colab_type": "text"
      },
      "source": [
        "# Libraies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svZWnIHwwNKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Necessary Libaries\n",
        "%matplotlib inline\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data\n",
        "from midi_utils import midiread, midiwrite\n",
        "from matplotlib import pyplot as plt\n",
        "import skimage.io as io\n",
        "from IPython.display import FileLink\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGNqzEZbwJyL",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Defined Parameters\n",
        "This is where we can define some model pramaters before we begin training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uys_7Qbuvbqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Customize Training Parameters Here\n",
        "# https://pytorch.org/docs/stable/nn.html for all param info\n",
        "CLIP = 3\n",
        "RNN_TYPE = \"gru\"\n",
        "DROPOUT = \"normal\"\n",
        "DROPOUT_RATE = .5\n",
        "OPTIMIZER_TYPE = \"adam\"\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "criterion_val = nn.CrossEntropyLoss().cuda()\n",
        "criterion_test = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "# Customize Outout MIDI File Here \n",
        "SAMPLE_LENGTH = 400\n",
        "TEMPERATURE = .8 \n",
        "DT = .2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss1aYU7G6mdm",
        "colab_type": "text"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZUDM_M80u5n",
        "colab_type": "text"
      },
      "source": [
        "This Section of code is responsible for converting the .midi files to a numeric representation.\n",
        "Code Taken Directly from https://github.com/jmcarpenter2/music-generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-27T12:20:52.867674Z",
          "start_time": "2018-11-27T12:20:52.819795Z"
        },
        "id": "60SOACKA6mdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "def midi_filename_to_piano_roll(midi_filename):\n",
        "    \n",
        "    midi_data = midiread(midi_filename, dt=0.3)\n",
        "    \n",
        "    piano_roll = midi_data.piano_roll.transpose()\n",
        "    \n",
        "    # Pressed notes are replaced by 1\n",
        "    piano_roll[piano_roll > 0] = 1\n",
        "    \n",
        "    return piano_roll\n",
        "\n",
        "\n",
        "def pad_piano_roll(piano_roll, max_length=132333, pad_value=0):\n",
        "        \n",
        "    original_piano_roll_length = piano_roll.shape[1]\n",
        "    \n",
        "    padded_piano_roll = np.zeros((88, max_length))\n",
        "    padded_piano_roll[:] = pad_value\n",
        "    \n",
        "    padded_piano_roll[:, -original_piano_roll_length:] = piano_roll\n",
        "\n",
        "    return padded_piano_roll\n",
        "\n",
        "\n",
        "class NotesGenerationDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, midi_folder_path, longest_sequence_length=1491):\n",
        "        \n",
        "        self.midi_folder_path = midi_folder_path\n",
        "        \n",
        "        midi_filenames = os.listdir(midi_folder_path)\n",
        "        \n",
        "        self.longest_sequence_length = longest_sequence_length\n",
        "        \n",
        "        midi_full_filenames = map(lambda filename: os.path.join(midi_folder_path, filename),midi_filenames)\n",
        "        \n",
        "        self.midi_full_filenames = list(midi_full_filenames)\n",
        "        \n",
        "        if longest_sequence_length is None:\n",
        "            \n",
        "            self.update_the_max_length()\n",
        "    \n",
        "    \n",
        "    def update_the_max_length(self):\n",
        "        \n",
        "        sequences_lengths = map(lambda filename: midi_filename_to_piano_roll(filename).shape[1],self.midi_full_filenames)\n",
        "        \n",
        "        max_length = max(sequences_lengths)\n",
        "        \n",
        "        self.longest_sequence_length = max_length\n",
        "                \n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.midi_full_filenames)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        midi_full_filename = self.midi_full_filenames[index]\n",
        "        \n",
        "        piano_roll = midi_filename_to_piano_roll(midi_full_filename)\n",
        "        \n",
        "        # Shifting by one time step\n",
        "        sequence_length = piano_roll.shape[1] - 1\n",
        "        \n",
        "        # Shifting by one time step\n",
        "        input_sequence = piano_roll[:, :-1]\n",
        "        ground_truth_sequence = piano_roll[:, 1:]\n",
        "                \n",
        "        # padding sequence so that all of them have the same length\n",
        "        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length)\n",
        "        \n",
        "        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,max_length=self.longest_sequence_length,pad_value=-100)\n",
        "                \n",
        "        input_sequence_padded = input_sequence_padded.transpose()\n",
        "        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n",
        "        \n",
        "        return (torch.FloatTensor(input_sequence_padded),torch.LongTensor(ground_truth_sequence_padded),torch.LongTensor([sequence_length]) )\n",
        "\n",
        "    \n",
        "def post_process_sequence_batch(batch_tuple):\n",
        "    \n",
        "    input_sequences, output_sequences, lengths = batch_tuple\n",
        "    \n",
        "    splitted_input_sequence_batch = input_sequences.split(split_size=1)\n",
        "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
        "    splitted_lengths_batch = lengths.split(split_size=1)\n",
        "\n",
        "    training_data_tuples = zip(splitted_input_sequence_batch,\n",
        "                               splitted_output_sequence_batch,\n",
        "                               splitted_lengths_batch)\n",
        "\n",
        "    training_data_tuples_sorted = sorted(training_data_tuples,\n",
        "                                         key=lambda p: int(p[2]),\n",
        "                                         reverse=True)\n",
        "\n",
        "    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
        "\n",
        "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
        "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
        "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
        "    \n",
        "    input_sequence_batch_sorted = input_sequence_batch_sorted[:, -lengths_batch_sorted[0, 0]:, :]\n",
        "    output_sequence_batch_sorted = output_sequence_batch_sorted[:, -lengths_batch_sorted[0, 0]:, :]\n",
        "    \n",
        "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
        "    \n",
        "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
        "    lengths_batch_sorted_list = map(lambda x: int(x), lengths_batch_sorted_list)\n",
        "    \n",
        "    return input_sequence_batch_transposed, output_sequence_batch_sorted, list(lengths_batch_sorted_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-27T12:21:07.678124Z",
          "start_time": "2018-11-27T12:20:56.931002Z"
        },
        "id": "BrC-Dpg46mdv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "b186b072-5885-449d-ad48-a51c3eb71126"
      },
      "source": [
        "# Load Training Data\n",
        "trainset = NotesGenerationDataset('./Kanye/train/', longest_sequence_length=None)\n",
        "trainset_loader = data.DataLoader(trainset, batch_size=8,shuffle=True, drop_last=True)\n",
        "\n",
        "# Get Training Data Shape\n",
        "X = next(iter(trainset_loader))\n",
        "print(\"Train Size:\",X[0].shape)\n",
        "print()\n",
        "\n",
        "# Load Validation Set\n",
        "valset = NotesGenerationDataset('./Kanye/valid/', longest_sequence_length=None)\n",
        "valset_loader = data.DataLoader(valset, batch_size=8, shuffle=False, drop_last=False)\n",
        "\n",
        "# Get Validation Set Size\n",
        "X_val = next(iter(valset_loader))\n",
        "print(\"Validation Size:\",X_val[0].shape)\n",
        "print()\n",
        "\n",
        "# Load Test Set\n",
        "testset = NotesGenerationDataset('./Kanye/test/', longest_sequence_length=None)\n",
        "testset_loader = data.DataLoader(testset, batch_size=8, shuffle=False, drop_last=False)\n",
        "\n",
        "# Get Test Set Size\n",
        "X_test = next(iter(testset_loader))\n",
        "print(\"Test Size:\",X_test[0].shape)\n",
        "print()\n",
        "\n",
        "# Load Full Data Set\n",
        "fulldata = NotesGenerationDataset('./Kanye/full_data/', longest_sequence_length=None)\n",
        "full_data_loader = data.DataLoader(fulldata, batch_size=8, shuffle=False, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Size: torch.Size([8, 2643, 88])\n",
            "\n",
            "Validation Size: torch.Size([8, 2643, 88])\n",
            "\n",
            "Test Size: torch.Size([8, 1842, 88])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNsNSHAh6mgH",
        "colab_type": "text"
      },
      "source": [
        "# RNN\n",
        "Here we build our RNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-27T12:22:33.314323Z",
          "start_time": "2018-11-27T12:22:33.291386Z"
        },
        "id": "Ry9-FNgn6mgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
        "        \n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "\n",
        "        # Linear Layer\n",
        "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
        "        \n",
        "        # Batch Normalization\n",
        "        self.bn = nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        # RNN (GRU or LSTM)\n",
        "        if RNN_TYPE == \"lstm\":\n",
        "          self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
        "        else:\n",
        "          self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "\n",
        "        # Linear Layer\n",
        "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        \n",
        "        # Dropout (Alpha or Regular)\n",
        "        if DROPOUT == \"alpha\":\n",
        "          self.dropout = nn.AlphaDropout(DROPOUT_RATE)\n",
        "        else:\n",
        "          self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "    \n",
        "    \n",
        "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
        "        batch_size = input_sequences.shape[1]\n",
        "\n",
        "        notes_encoded = self.notes_encoder(input_sequences)\n",
        "        \n",
        "        \n",
        "        # Change Axis\n",
        "        notes_encoded_rolled = notes_encoded.permute(1,2,0).contiguous()\n",
        "        \n",
        "        # Batch Normalization\n",
        "        notes_encoded_norm = self.bn(notes_encoded_rolled)\n",
        "        \n",
        "        # Dropout\n",
        "        notes_encoded_norm_drop = self.dropout(notes_encoded_norm)\n",
        "        \n",
        "        # Return to axis\n",
        "        notes_encoded_complete = notes_encoded_norm_drop.permute(2,0,1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Here we run rnns only on non-padded regions of the batch\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(notes_encoded_complete, input_sequences_lengths)\n",
        "        \n",
        "        # RNN\n",
        "        outputs, hidden = self.rnn(packed, hidden)\n",
        "        \n",
        "        # Here we unpack sequence(back to padded)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        \n",
        "        # Batch Normalization\n",
        "        outputs_norm = self.bn(outputs.permute(1,2,0).contiguous())\n",
        "        \n",
        "        # Dropout\n",
        "        outputs_drop = self.dropout(outputs_norm)\n",
        "        \n",
        "        # Linear Layer\n",
        "        logits = self.logits_fc(outputs_drop.permute(2,0,1))\n",
        "        logits = logits.transpose(0, 1).contiguous()\n",
        "        \n",
        "        neg_logits = (1 - logits)\n",
        "        \n",
        "        # Since the BCE loss doesn't support masking,crossentropy is used\n",
        "        binary_logits = torch.stack((logits, neg_logits), dim=3).contiguous()\n",
        "        logits_flatten = binary_logits.view(-1, 2)\n",
        "        return logits_flatten, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MA18J6H1tRv",
        "colab_type": "text"
      },
      "source": [
        "# Parameter Search\n",
        "This is where we try out different parameters. We loop through different hidden layer values, epochs, and learning rates and use the validation set as the way to determine what parameters gave us the best results using Cross Entropy Loss. We also save the optimal model that gave us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjYqqqy3fOHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the model on the validation set\n",
        "def validate(model):\n",
        "    \n",
        "    # Set to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    # Keep a track of the loss\n",
        "    full_val_loss = 0.0\n",
        "    overall_sequence_length = 0.0\n",
        "\n",
        "\n",
        "    # Loop through the batches\n",
        "    for batch in valset_loader:\n",
        "\n",
        "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "\n",
        "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
        "\n",
        "        output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n",
        "\n",
        "        input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n",
        "\n",
        "        logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
        "\n",
        "        loss = criterion_val(logits, output_sequences_batch_var)\n",
        "\n",
        "        full_val_loss += loss.item()\n",
        "        overall_sequence_length += sum(sequences_lengths)\n",
        "\n",
        "    return full_val_loss / (overall_sequence_length * 88)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkfZBzJ_VWJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that determines what parameters we should be using.\n",
        "def lrfinder(start, end, trainset_loader):\n",
        "    \n",
        "    # Different Hidden Values\n",
        "    hiddens = [16, 32, 64 ,128, 256, 512]\n",
        "    \n",
        "    # Different Learning Rates\n",
        "    learning_rates = np.linspace(start, end, 4)\n",
        "    \n",
        "    # Max Epochs to test out\n",
        "    epochs = 20\n",
        "\n",
        "\n",
        "    # Used to let us know how far along this cell will take\n",
        "    denom = (epochs)*len(hiddens)*len(learning_rates)\n",
        "\n",
        "\n",
        "    # Initialize all parameters \n",
        "    best_epoch = 0\n",
        "    best_hidden = 0\n",
        "    best_lr = 0\n",
        "    best_val_loss = float(\"inf\")\n",
        "    train_loss = float(\"inf\")\n",
        "    counter = 0\n",
        "    \n",
        "    # Loop through different learning rates and hidden layers\n",
        "    for hidden in hiddens:\n",
        "      for rate in learning_rates:\n",
        "        \n",
        "        # Let the user know how much percent this cell is complete\n",
        "        print(\"Percent Complete: {}%\".format((counter / denom) * 100))\n",
        "        \n",
        "        # Create the Model and set to training mode\n",
        "        model = RNN(input_size=88, hidden_size=hidden, num_classes=88)\n",
        "        model = model.cuda()\n",
        "        model.train()\n",
        "        \n",
        "        parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "        \n",
        "        # Pick the Optimzier the user defines\n",
        "        if OPTIMIZER_TYPE == \"sgd\":\n",
        "          optimizer = torch.optim.SGD(model.parameters(), lr=rate)\n",
        "        else:\n",
        "          optimizer = torch.optim.Adam(model.parameters(), lr=rate)\n",
        "\n",
        "        # Loop through the epochs (20 in this case)\n",
        "        for i in range(epochs):\n",
        "          model.train()\n",
        "          \n",
        "          # Keep a track of the loss per epoch\n",
        "          epoch_loss = [] \n",
        "          \n",
        "          # Loop through the batches\n",
        "          for batch in trainset_loader:\n",
        "            \n",
        "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
        "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n",
        "            input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
        "\n",
        "            # Get the Loss\n",
        "            loss = criterion(logits, output_sequences_batch_var)\n",
        "            epoch_loss.append(loss.item())\n",
        "   \n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the Gradients \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "\n",
        "            optimizer.step()\n",
        "            \n",
        "          # Add to our completion percentage   \n",
        "          counter = counter + 1\n",
        "          \n",
        "          \n",
        "          # Get the validation after one pass through of the data\n",
        "          val_loss = validate(model)\n",
        "          \n",
        "          # If the validation loss is better, update the params and save the model\n",
        "          if val_loss < best_val_loss:\n",
        "            best_epoch = i\n",
        "            best_hidden = hidden\n",
        "            best_lr = rate\n",
        "            best_val_loss = val_loss\n",
        "            train_loss = sum(epoch_loss)/len(trainset_loader)\n",
        "            \n",
        "            # Save the model for the next step\n",
        "            torch.save(model.state_dict(), 'music_model_padfront_regularized.pth')\n",
        "\n",
        "\n",
        "\n",
        "    # Return the params that gave the optimal loss\n",
        "    return best_epoch, best_hidden, best_lr, best_val_loss, train_loss\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfGwjQs5gk-Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "780d02fb-ef00-42a6-e24e-3e8677c28818"
      },
      "source": [
        "# Find the best params and save the results and model\n",
        "best_epoch, best_hidden_size, best_learning_rate, val_loss, train_loss = lrfinder(1e-4, 1e-1*5,trainset_loader)\n",
        "print(\"Best Epoch:\",best_epoch, \"  Best Hidden Size:\",best_hidden_size, \"  Best Learning Rate:\",best_learning_rate) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent Complete: 0.0%\n",
            "Percent Complete: 4.166666666666666%\n",
            "Percent Complete: 8.333333333333332%\n",
            "Percent Complete: 12.5%\n",
            "Percent Complete: 16.666666666666664%\n",
            "Percent Complete: 20.833333333333336%\n",
            "Percent Complete: 25.0%\n",
            "Percent Complete: 29.166666666666668%\n",
            "Percent Complete: 33.33333333333333%\n",
            "Percent Complete: 37.5%\n",
            "Percent Complete: 41.66666666666667%\n",
            "Percent Complete: 45.83333333333333%\n",
            "Percent Complete: 50.0%\n",
            "Percent Complete: 54.166666666666664%\n",
            "Percent Complete: 58.333333333333336%\n",
            "Percent Complete: 62.5%\n",
            "Percent Complete: 66.66666666666666%\n",
            "Percent Complete: 70.83333333333334%\n",
            "Percent Complete: 75.0%\n",
            "Percent Complete: 79.16666666666666%\n",
            "Percent Complete: 83.33333333333334%\n",
            "Percent Complete: 87.5%\n",
            "Percent Complete: 91.66666666666666%\n",
            "Percent Complete: 95.83333333333334%\n",
            "Best Epoch: 13   Best Hidden Size: 16   Best Learning Rate: 0.16673333333333332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUeyqJ5Qfz5f",
        "colab_type": "text"
      },
      "source": [
        "## Test Set\n",
        "With our optimal model, we check to see how this model does on the test set. If the test loss isn't too much higher than the validation loss, we continue. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3smmZA0rAw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "008d5672-6751-4e55-e54f-c2df3238a08f"
      },
      "source": [
        "# Load the optimal model we created from the previous step\n",
        "test_model = RNN(input_size=88, hidden_size=best_hidden_size, num_classes=88)\n",
        "test_model = test_model.cuda()\n",
        "test_model.load_state_dict(torch.load('music_model_padfront_regularized.pth'))\n",
        "test_model.eval()\n",
        "\n",
        "# Initilize the Test Loss\n",
        "full_test_loss = 0.0\n",
        "overall_sequence_length = 0.0\n",
        "\n",
        "# Loop through the batches\n",
        "for batch in testset_loader:\n",
        "\n",
        "    post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "\n",
        "    input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
        "\n",
        "    output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n",
        "\n",
        "    input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n",
        "\n",
        "    logits, _ = test_model(input_sequences_batch_var, sequences_lengths)\n",
        "\n",
        "    loss = criterion_test(logits, output_sequences_batch_var)\n",
        "\n",
        "    full_test_loss += loss.item()\n",
        "    overall_sequence_length += sum(sequences_lengths)\n",
        "\n",
        "# Show the Validation and Test Loss of this model\n",
        "print(\"Validation Loss\", val_loss)\n",
        "print(\"Test Loss\", full_test_loss / (overall_sequence_length * 88))    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Loss 6.622388702578636e-07\n",
            "Test Loss 8.733955039356646e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXaRNiClfn2J",
        "colab_type": "text"
      },
      "source": [
        "## Model Training\n",
        "Now that we know what parameters to use, we build the model using the optimal parameters on the entire dataset (train, valid, test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-26T16:16:18.949227Z",
          "start_time": "2018-11-26T16:16:18.930281Z"
        },
        "id": "N-P0lhYV6mg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(learning_rate, epochs_number, hidden):\n",
        "    model = RNN(input_size=88, hidden_size=hidden, num_classes=88)\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "    \n",
        "    \n",
        "    # Determine which Optimzier to use \n",
        "    if OPTIMIZER_TYPE == \"sgd\":\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    else:\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "\n",
        "    # Loop through different Epoch values    \n",
        "    for epoch_number in range(epochs_number):\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        # Loop through different Learning rates \n",
        "        for batch in full_data_loader:\n",
        "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
        "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1).cuda() )\n",
        "            input_sequences_batch_var = Variable( input_sequences_batch.cuda() )\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
        "\n",
        "            # Get Loss\n",
        "            loss = criterion(logits, output_sequences_batch_var)\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip Gradient\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "    torch.save(model.state_dict(), 'final_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp6eTR6X0IYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_model(best_learning_rate, best_epoch, best_hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEafC2686mhW",
        "colab_type": "text"
      },
      "source": [
        "# Music Generation\n",
        "Now that we have finished training our RNN, we can now use it to generate music! The generated clip will be in this working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-26T16:25:11.202962Z",
          "start_time": "2018-11-26T16:25:11.189993Z"
        },
        "id": "6Fjx2hfD6mhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_from_piano_rnn(rnn, sample_length=4, temperature=1, starting_sequence=None):\n",
        "\n",
        "    # Arbitrary Starting Point\n",
        "    if starting_sequence is None:\n",
        "\n",
        "        # Initialize the current sequence         \n",
        "        current_sequence_input = torch.zeros(1, 1, 88)\n",
        "        current_sequence_input[0, 0, 40] = 1\n",
        "        current_sequence_input[0, 0, 50] = 0\n",
        "        current_sequence_input[0, 0, 56] = 0\n",
        "        current_sequence_input = Variable(current_sequence_input.cuda())\n",
        "    \n",
        "    # If we have a given starting point\n",
        "    else:\n",
        "        current_sequence_input = starting_sequence\n",
        "        \n",
        "    # Create the output sequence \n",
        "    final_output_sequence = [current_sequence_input.data.squeeze(1)]\n",
        "\n",
        "    hidden = None\n",
        "\n",
        "    # Sample For the desired length \n",
        "    for i in range(sample_length):\n",
        "\n",
        "        # Use the current sequence to generate a new sequence \n",
        "        output, hidden = rnn(current_sequence_input, [1], hidden)\n",
        "\n",
        "        # Get the probabilites of possible notes from the output and temperature\n",
        "        probabilities = nn.functional.softmax(output.div(temperature), dim=1)\n",
        "\n",
        "        # Sample from the multinomial distribution and set this new sequence to be current sequence \n",
        "        current_sequence_input = torch.multinomial(probabilities.data, 1).squeeze().unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "        # Returns tensor \n",
        "        current_sequence_input = Variable(current_sequence_input.float())\n",
        "\n",
        "        # Append this current sequence to our entire output\n",
        "        final_output_sequence.append(current_sequence_input.data.squeeze(1))\n",
        "\n",
        "    \n",
        "    # Finalize the Output and return\n",
        "    sampled_sequence = torch.cat(final_output_sequence, dim=0).cpu().numpy()\n",
        "    \n",
        "    return sampled_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-11-26T16:25:36.632928Z",
          "start_time": "2018-11-26T16:25:34.966547Z"
        },
        "id": "xxu7fQDX6mi2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "aaa26e89-e961-4f51-8c1a-4e393d202d1d"
      },
      "source": [
        "final_model = RNN(input_size=88, hidden_size=best_hidden_size, num_classes=88)\n",
        "final_model = final_model.cuda()\n",
        "final_model.load_state_dict(torch.load('final_model.pth'))\n",
        "final_model.eval()\n",
        "sample = sample_from_piano_rnn(final_model, sample_length=SAMPLE_LENGTH, temperature=TEMPERATURE,starting_sequence=None).transpose()\n",
        "io.imshow(sample)\n",
        "midiwrite(RNN_TYPE+\" Clip:\"+str(CLIP)+\" Epochs:\"+str(best_epoch)+\" Optimizer:\"+OPTIMIZER_TYPE+\" Hidden:\"+str(best_hidden_size)+\" Dropout:\"+str(DROPOUT)+\" Dropout Rate:\"+str(DROPOUT_RATE)+\" Length:\"+str(SAMPLE_LENGTH)+\" Temperature:\"+str(TEMPERATURE)+\" DT:\"+str(DT)+\".mid\", sample.transpose(), dt=DT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}