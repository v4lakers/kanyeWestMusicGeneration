<!DOCTYPE HTML>
<!--
	Visualize by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bebas+Neue">
		<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open Sans">
		<title>Kanye Music Generation</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<style>
			audio{
				width: 225px;
				vertical-align: middle;

			}
			body {
        		font-family: 'Bebas Neue';
      			}

			ol, p{
				font-family: "Open Sans";
				font-style: normal;
				font-variant: normal;
			}
			a{
				color: lightblue;
			}

			.center {
  			display: block;
  			margin-left: auto;
  			margin-right: auto;
					}


		</style>

	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<span class="avatar"><img src="assets/images/avatar.jpg" alt="" /></span>
						<h1><strong>Kanye West </strong> Music Generation</h1>
						<h2>By Vivek Sharath</h2>

						<ul class="icons">
							<li><span class="album"><img src="assets/albums/college.png" alt="" /></span></li>
							<li><span class="album"><img src="assets/albums/late.png" alt="" /></span></li>
							<li><span class="album"><img src="assets/albums/grad.png" alt="" /></span></li>
							<li><span class="album"><img src="assets/albums/808.png" alt="" /></span></li>
							<li><span class="album"><img src="assets/albums/mbdtf.png" alt="" /></span></li>
							<li><span class="album"><img src="assets/albums/yeezus.png" alt="" /></span></li>
							<li><span class="album"><img src="assets/albums/tlop.png" alt="" /></span></li>
							<li><span class="album"><img src="assets/albums/ye.png" alt="" /></span></li>
							<li><span class="album"><img src="assets/albums/jik.png" alt="" /></span></li>
						</ul>


					</header>

				<!-- Main -->
				<h1>Overview</h1>
				<p>This project explains how I trained a model to go from this &nbsp <audio controls >
            		<source src="Results/Trials/trial1.mp3" type="audio/wav" >
        		</audio> &nbsp into this &nbsp
				<audio controls>
            		<source src="Results/Beats_made_with_Best_Result/Knock_Off_Donkey_Kong/Knock_off_Donkey_Kong.wav" type="audio/wav">
        		</audio><br><br>

					For details on code, please visit the <a href = "https://github.com/v4lakers/kanyeWestMusicGeneration">GitHub Repository</a>.
					For all acknowledgements for this project, please check out the <a href = "https://docs.google.com/document/d/1mv6ZL3bcY1lyZmkU2nr1XybpBQhd2-bzV8YrhheQuNs/edit?usp=sharing">acknowledgements page</a>.
				</p>
				<br><br>
				<h1>Introduction</h1>
				<p align="justify"> From the uplifting beats featured in his debut album “The College Dropout”,
					to the unorthodox blend of synthetics and vocals of “Graduation”, to the christian hymns of “Jesus is King”,
					Kanye West continues to defy established musical patterns while also creating melodies that have rocked the 21st century.
					In this project, we will be using RNN’s (recurrent neural networks) to generate music that aims to mimic his style.
				</p>
				<br><br>
				<h1>How Does This Work?</h1>
				<p align="justify">RNN and music generation go together like peanut butter and jelly. RNN's take in sequential data (say music for example) as input and tries to look for some kind of pattern.
					If our model finds this underlying pattern of the sequential data, we can use the model to predict what
					should come next given some arbitrary starting point. In the case of music generation, we want to predict the next sound.</p>
				<img src="assets/images/rnn.png" style="width:80%;" class="center">
				<br><br>
				<h1>Piano Music</h1>
				<p align="justify"> Spoiler Alert: we will be using piano covers of Kanye West's music for this project. Why? Short answer: it's easier.
					 When it comes to piano music, it is clear which notes are being played at any given time. How do we know which notes are
					 being played at a given time? ~cue MIDI files~. Midi files, also known as musical instrument digital interface,
					 is essentially a sequence of notes over a time period. The image below is what a .midi file looks like. </p>
				<img src="assets/images/midi.png" style="width:80%;" class="center">
				<br>
				<p align="justify">You can think of this as the “technological” way of reading piano sheet music. The x axis (length of the song)
					acts as the time while the y axis (size 88 because there are 88 playable notes) lets us know which notes are being
					played. We can turn this into a matrix of size 88 x tn where tn is the length of the song. For example,
					the column t91 will tell us which of the 88 notes are being played at time step 91.

					Once we have our tn columns, we feed each column ti , 1 ≤ i ≤ n, into our model one at a time.
					Note that the vector ti can have multiple 1’s throughout the vector. One could find a way to encode
					this vector, but for the sake of space, inputting an 88x1 vector will work as well.
				</p>

				<img src="assets/images/matrix.png" style="width:80%;" class="center">
				<br><br>
				<h1>Data</h1>
				<p align="justify">
					This is where the fun begins. As mentioned earlier, piano covers for various Kanye West songs
					populate my dataset. Did I do these covers on my own? Nope. Instead, I converted piano covers
					of Kanye's songs on youtube to midi files. (Links to all the youtubers who supplied the beautiful covers
					can be found in the acknowledgements page). Once I had about 100 minutes of Kanye Piano Cover midi files,
					I split the data into a train set (50%), test set (25%), and validation set (25%).
				</p>
				<img src="assets/images/process.png" style="width:80%;" class="center">
				<br><br>
				<h1>Code</h1>
				<p align="justify">
					This is where the fun really begins. Huge huge huge thank you to James Carpenter and Prince Grover who
					developed an ingenious pipeline that allows for midi files to be used in the forward and backward
					steps of deep learning. This project would not exist without their code (link to their work in the github
					page for this project and in the acknowledgments page).
					<br><br>
					Here is a brief overview of what our code does<br>
					<ol>
						<li>🔢 Load in our train, test, validation sets 🔢</li>
						<li>💪 Train various models using different pre-training parameters (hidden size, epochs, learning rates) 💪 </li>
						<li>🥇Choose the model that gave us the best validation score, and save the pre-training parameters 🥇</li>
						<li>😬 Do a sanity check by testing our model against our untouched test set 😬</li>
						<li>🏗 Build a new model with the saved pre-training parameters 🏗</li>
						<li>💪 Train this model with the entire dataset (train, test, valid) 💪</li>
						<li>🎵😎🎵 Use this model to generate some epic tunes 🎵😎🎵</li>
					</ol>
				</p>
				<br>
				<h1>Results</h1>
				<p align="justify">
					Remember step 7 from above? It took several trial and errors to get anywhere near "epic". Instead of showing
					all 200 something trials, the following 4 tracks show the progression of my quest towards making music like Kanye.
					If you are a fan of scary movie soundtracks or a cacophony of sounds, you are in for a treat!
				</p>
				<code>
					Results During Parameter Tuning
				</code>
				<br><br>
					<audio controls>
            		<source title="trial 1" src="Results/Trials/trial1.mp3" type="audio/mp3">
        		</audio>
					<audio controls>
            		<source title="trial 50" src="Results/Trials/trial50.mp3" type="audio/mp3">

        		</audio>
				<br><br>
					<audio controls>
            		<source title="trial 100" src="Results/Trials/trial100.mp3" type="audio/mp3">
        		</audio>
					<audio controls>
            		<source title="trial 200" src="Results/Trials/trial200.mp3" type="audio/mp3">
        		</audio>
				<br><br><br>
				<p align="justify">
					Yikes. What makes this project so hard is that relying on the loss isn't the best indicator
					if we are producing good music or not. After hours and hours of changing different model parameters,
					namely different levels of dropout and RNN architecture, I finally arrived at a generated audio clip
					that had a noticeable melody and was free of piano banging.
				</p>

				<code>Good Sounding Result</code>
				<br><br>
				<audio controls>
            		<source src="Results/Best_Sounding_Result/og.wav" type="audio/wav">
        		</audio>
				<br><br><br>
				<h1>Turn Up!</h1>
				<p align="justify">
					Using the "Good Sounding Generated Clip" as the underlying beat, my brother was able to add a bassline
					and a few other instruments to produce the following beats. Enjoy!
				</p>
				<code>Beat 1: Someone's At Your Door</code>
				<br><br>
				<audio controls>
            		<source src="Results/Beats_made_with_Best_Result/Someone's_At_Your_Door/Someone's_at_your_door_lmao.wav" type="audio/wav">
        		</audio>
				<br><br>
				<img src="assets/images/lmao.png" style="width:55%;">
				<br>
				<br>
				<br>
				<br>
				<code>Beat 2: Knock off Donkey Kong</code>
				<br><br>
				<audio controls>
            		<source src="Results/Beats_made_with_Best_Result/Knock_Off_Donkey_Kong/Knock_off_Donkey_Kong.wav" type="audio/wav">
        		</audio>
				<br><br>
				<img src="assets/images/dk.png" style="width:55%;">



				<!-- Footer -->
					<footer id="footer">
						<p><a href="https://templated.co/">Design: TEMPLATED</a></p>
					</footer>

			</div>



	</body>
</html>